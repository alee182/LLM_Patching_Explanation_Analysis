# LLM_Patching_Explanation_Analysis
Introduction
The rise of large language models (LLMs) has transformed the way developers interact with code and software documentation, enabling the automated generation of annotations and natural language summaries. These capabilities are particularly valuable in the domain of software security, where the use of LLMs to assess and patch vulnerabilities in software may become common practice in the future. However, the quality of LLM-generated outputs, particularly their accuracy, comprehensiveness, and consistency, remains underexplored as of now. Inaccurate or incomplete annotations can lead to misinterpretations and missed opportunities for remediation, highlighting the need for a deeper investigation into the strengths and limitations of LLMs in this critical area. Through this research we will analyze LLM’s ability to generate comprehensive and consistent descriptions of LLM generated patches

Research Questions
RQ1: How  accurately can large language models (LLMs) generate natural language explanations of software patches for vulnerabilities on a consistent basis?
RQ2:  How accurately can LLMs identify the CWE ID of a particular vulnerability?
RQ3:What role does the level of detail in the input context play in the quality of LLM-generated patch explanations?

Set-up
Dataset
The dataset we will be utilizing in this research is the Bigvul Dataset. This dataset is an open source collection of C/C++ vulnerabilities with a total of 3754 vulnerable  functions. Each function within the dataset contains both a vulnerable and fixed version of the code as well as meta data that includes the CWE ID , link to CWE explanation, Commit Message, etc. 
LLMs
To get an accurate overall assessment of LLMs we have decided to assess three different pre-trained models. LLama3.1, Mistral, and Deepseek V3. These models have vastly different training data and parameters used in their creation. The comparison of Overall score between all of these models will help determine which pre-trained models are best suited for vulnerability patching. 
Tools
SentenceBERT (SBERT) is a variant of BERT fine-tuned to generate dense vector embeddings for sentences, enabling efficient and meaningful comparison between text pairs. By encoding sentences into fixed-size embeddings, SBERT allows quick computation of similarity scores in the context of semantic text comparison. For the purpose of this research, SBERT will be used to  identify how close the Natural Language summary produced by the LLM is to Metadata for Vulnerabilities by creating embeddings that will be then compared and quantified through Cosine similarity scoring.
Cosine similarity is a metric used to measure the similarity between two vectors by calculating the cosine of the angle between them. In the context of text comparison, each piece of text is represented as a vector in a high-dimensional space, often derived from embeddings like those generated by SentenceBERT. A cosine similarity score, ranging from -1 (opposite) to 1 (identical), quantifies how closely the meanings of the text and reference text align.
ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate the quality of text by comparing it to a reference text, primarily in summarization tasks. It works by measuring overlaps between the generated text and the reference text in terms of n-grams, word sequences, or longest common subsequences. For the purpose of this research ROUGE will be used to produce F1 scores when comparing the LLM generated summary to CWE information. 
UniEval is a unified evaluation framework for assessing text generation quality across various tasks, including summarization, translation, and dialogue generation. It combines multiple evaluation dimensions, such as coherence, fluency, relevance, and factual consistency, by leveraging both reference-based and reference-free approaches For the purpose of this research the UniEval will be utilized to assess the quality of LLM generated summaries. 
Methodology
To analyze LLMs ability to generate accurate assessments of vulnerability patching We’ve established a workflow that gives LLMs necessary vulnerability information, Then prompts the LLM to answer specific questions about its analysis. We then utilize several types of automated data analysis and create an overall assessment score of LLMs ability to explain why a patch was made.
The information that the LLM will be given includes, The original version of the function, the fixed version of the function and the commit message. To study the impact of how much context is given 50% of the code functions will be given with the commit message and 50% will be given without. The LLM will then be prompted to respond with the 3 most likely CWE IDs that were fixed in the function in order of likeliness, A natural Language summary based on the changes made and the reasoning of why. And a commit message.

The information generated by the LLM will be analyzed and quantified by the tools listed above to create an overall assessment of the LLMs understanding of the patchwork. To create this overall score a holistic approach has been taken to combine the scoring from each tool with an order of importance. The most important factor that will be taken into account is based on If the LLM identified the correct CWE ID. The scoring from Cosine Similarity, and ROUGE will be accounted for with high priority as they are good indicators of accuracy from the model. The UNIEVAL score will also be taken into account with lower priority as its scoring is based largely on quality of the assessment and not accuracy. 

Next Step
Since a research outline has been created, our first step is to create a small scale program to further assess if this methodology is viable and if any tweaks are needed before we create an automated program to run through the entire dataset as well serve as a baseline for the automated program. This program will utilize LLama3.1 as it is a model we have prior experience with and will need the data from the dataset to be manually collected. The program will give LLama 3.1 a prompt with the fixed function, vulnerable function and commit message, and ask for a natural language summary and CWE ID and 2 secondary guesses for CWE ID if the first one is incorrect. All of the tools listed above will be used to analyze LLM generated responses and their comparisons  to CWE  ID explanations when appropriate. Some features that are planned to be used within the automated program that won’t be included in the miniature version include the combination of scoring as well as the collection of scores in designated JSON files.

